{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping data from OERCOMMONS"
      ],
      "metadata": {
        "id": "9s6DG2rpVS93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for scraping\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional, List\n",
        "\n",
        "# Helper Functions for Scraping\n",
        "\n",
        "def get_element_text(element, index: int = 0, default: str = 'N/A') -> str:\n",
        "\n",
        "    try:\n",
        "        value = element.find_all('dd')[index].text.strip()\n",
        "        return value if value else default\n",
        "    except (AttributeError, IndexError):\n",
        "        return default\n",
        "\n",
        "def get_metadata_content(soup, meta_name: str, default: str = 'N/A') -> str:\n",
        "\n",
        "    try:\n",
        "        return soup.find('meta', {'itemprop': meta_name})['content'].strip()\n",
        "    except (AttributeError, TypeError):\n",
        "        return default\n",
        "\n",
        "# Function to Fetch Course Data\n",
        "\n",
        "def scrape_course_data(course, base_url: str) -> Optional[dict]:\n",
        "\n",
        "    try:\n",
        "        # Fetch course-specific information\n",
        "        title = course.find('div', class_='item-title').text.strip()\n",
        "        link = course.find('a', class_='item-link')['href']\n",
        "        author = get_element_text(course, index=0)\n",
        "        subject = get_element_text(course, index=1)\n",
        "        material_type = get_element_text(course, index=2)\n",
        "        description = course.find('div', class_='abstract').text.strip() if course.find('div', 'abstract') else 'N/A'\n",
        "\n",
        "        # Fetch the lesson URL and parse the details\n",
        "        lesson_url = base_url + link\n",
        "        response = requests.get(lesson_url)\n",
        "        response.raise_for_status()\n",
        "        lesson_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        keywords = get_metadata_content(lesson_soup, 'keywords')\n",
        "        date_created = get_metadata_content(lesson_soup, 'dateCreated')\n",
        "        resource_url = lesson_soup.find('a', {'id': 'goto'})['href'] if lesson_soup.find('a', {'id': 'goto'}) else 'N/A'\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"author\": author,\n",
        "            \"subject\": subject,\n",
        "            \"material_type\": material_type,\n",
        "            \"description\": description,\n",
        "            \"keywords\": keywords,\n",
        "            \"date_created\": date_created,\n",
        "            \"resource_url\": resource_url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing course data: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "9YIuARZ2VTOX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pydantic Validation and Data Handling"
      ],
      "metadata": {
        "id": "NoIxoGdWX2yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Likewise, importing necesarry libraries for Pydantic work\n",
        "\n",
        "from pydantic import BaseModel, HttpUrl, TypeAdapter\n",
        "import json\n",
        "\n",
        "# Define Pydantic Model for Lessons\n",
        "\n",
        "class Lesson(BaseModel):\n",
        "\n",
        "    title: str\n",
        "    author: Optional[str]\n",
        "    subject: str\n",
        "    material_type: str\n",
        "    description: Optional[str]\n",
        "    keywords: Optional[str]\n",
        "    date_created: Optional[str]\n",
        "    resource_url: HttpUrl\n",
        "\n",
        "# Function to Scrape All Lessons\n",
        "\n",
        "def scrape_lessons(base_url: str, search_url: str) -> List[dict]:\n",
        "\n",
        "    try:\n",
        "        response = requests.get(search_url)\n",
        "        response.raise_for_status()\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching search URL: {e}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    courses = soup.find_all('article', class_='js-index-item')\n",
        "\n",
        "    lesson_data = []\n",
        "\n",
        "    for course in courses:\n",
        "        lesson = scrape_course_data(course, base_url)\n",
        "        if lesson:\n",
        "            lesson_data.append(lesson)\n",
        "\n",
        "    return lesson_data\n",
        "\n",
        "# Function to Save Data to JSON\n",
        "\n",
        "def save_data_to_json(data, file_name='lessons_sync.json'):\n",
        "\n",
        "    with open(file_name, 'w') as f:\n",
        "        # Convert HttpUrl fields to strings for serialization\n",
        "        json_compatible_data = [lesson.dict() for lesson in data]\n",
        "        for lesson in json_compatible_data:\n",
        "            lesson[\"resource_url\"] = str(lesson[\"resource_url\"])\n",
        "        json.dump(json_compatible_data, f, indent=4)\n",
        "    print(f\"Data saved to {file_name}\")"
      ],
      "metadata": {
        "id": "TkE2EBjeVePP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNQI6Ksmb90G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}